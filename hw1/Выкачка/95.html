<!DOCTYPE html>
<html data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D" lang="ru">
<head>



<title>GAN: убийство двух зайцев одним выстрелом для синтеза табличных данных / Хабр</title>














































</head>
<body>
<div data-async-called="true" data-server-rendered="true" id="app"><div class="tm-layout__wrapper"><!-- --> <div></div> <div class="tm-feature tm-feature"><!-- --></div> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><div class="tm-header__burger-nav"><button class="tm-header__button tm-header__button_burger" type="button"><svg class="tm-svg-img tm-header__icon tm-header__icon-burger" height="16" width="16"><title>Меню</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#header-burger"></use></svg></button></div> <span class="tm-header__logo-wrap"><a class="tm-header__logo tm-header__logo_ru" href="/ru/"><svg class="tm-svg-img tm-header__icon" height="16" width="16"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <!-- --> <div class="tm-header-user-menu tm-header_user-menu"><a class="tm-header-user-menu__item tm-header-user-menu__search" href="/ru/search/"><svg class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search" height="24" width="24"><title>Поиск</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#search"></use></svg></a> <!-- --> <!-- --> <!-- --> <div class="tm-header-user-menu__item"><button class="tm-header-user-menu__toggle" data-test-id="menu-toggle-guest"><svg class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_white" height="24" width="24"><title>Профиль</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#header-user"></use></svg></button> <!-- --></div> <!-- --></div></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <!-- --> <!-- --> <div class="tm-page-width"></div> <main class="tm-layout__container"><div class="tm-page" data-async-called="true" hl="ru"><div class="tm-page-width"><!-- --> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><!-- --> <div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg class="tm-svg-img pull-down__arrow" height="24" width="24"><title>Обновить</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet tm-article-snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a class="tm-user-info__userpic" href="/ru/users/AyratGil/" title="AyratGil"><div class="tm-entity-image"><svg class="tm-svg-img tm-image-placeholder tm-image-placeholder_pink" height="32" width="32"><!-- --> <use xlink:href="/img/megazord-v28.617e16ca..svg#placeholder-user"></use></svg></div></a> <span class="tm-user-info__user"><a class="tm-user-info__username" href="/ru/users/AyratGil/">
      AyratGil
      <!-- --></a> <span class="tm-article-datetime-published"><time datetime="2023-03-09T15:40:02.000Z" title="2023-03-09, 18:40">9  мар   в 18:40</time></span></span></span></div> <!-- --></div> <h1 class="tm-article-snippet__title tm-article-snippet__title_h1" lang="ru"><span>GAN: убийство двух зайцев одним выстрелом для синтеза табличных данных</span></h1> <div class="tm-article-snippet__stats"><div class="tm-article-complexity tm-article-complexity_complexity-low"><span class="tm-svg-icon__wrapper tm-article-complexity__icon"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Уровень сложности</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#complexity-low"></use></svg></span> <span class="tm-article-complexity__label">
    Простой
  </span></div> <div class="tm-article-reading-time"><span class="tm-svg-icon__wrapper tm-article-reading-time__icon"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Время на прочтение</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#clock"></use></svg></span> <span class="tm-article-reading-time__label">
    22 мин
  </span></div> <span class="tm-icon-counter tm-data-icons__item"><svg class="tm-svg-img tm-icon-counter__icon" height="24" width="24"><title>Количество просмотров</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-views"></use></svg> <span class="tm-icon-counter__value">663</span></span></div> <div class="tm-article-snippet__hubs-container"><div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a class="tm-article-snippet__hubs-item-link" href="/ru/hub/algorithms/"><span>Алгоритмы</span> <span class="tm-article-snippet__profiled-hub" title="Профильный хаб">*</span></a></span><span class="tm-article-snippet__hubs-item"><a class="tm-article-snippet__hubs-item-link" href="/ru/hub/machine_learning/"><span>Машинное обучение</span> <span class="tm-article-snippet__profiled-hub" title="Профильный хаб">*</span></a></span></div></div> <div class="tm-article-snippet__labels-container"><div class="tm-article-snippet__labels"><!-- --> <div class="tm-article-snippet__label tm-article-snippet__label tm-article-snippet__label_variant-translation"><span>
          Перевод
        </span></div></div></div> <!-- --> <!-- --></div></div> <div class="tm-article-presenter__origin"><a class="tm-article-presenter__origin-link" href="https://arxiv.org/pdf/2202.03636.pdf" target="_blank">
                Автор оригинала:
                <span>
                  Jaehoon Lee, Jihyeon Hyeong, Jinsung Jeon, Noseong Park, Jihoon Cho
                </span></a></div> <div class="tm-article-body" data-gallery-root="" lang="ru"><div></div> <div id="post-content-body"><div><div class="article-formatted-body article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><h2>Аннотация</h2><p>Синтез табличных данных получил широкое внимание в литературе. Это связано с тем, что доступные данные часто ограничены, неполны или не могут быть легко получены, а конфиденциальность данных становится все более актуальной. В этой работе мы представляем обобщенную структуру генеративной состязательной сети (GAN) для табличного синтеза, которая сочетает в себе состязательное обучение и регуляризацию при отрицательной логарифмической плотности обратимых нейронных сетей. Предлагаемая структура может быть использована для достижения двух различных целей. Во‑первых, мы можем далее улучшить качество синтеза, уменьшив отрицательную логарифмическую плотность реальных записей в процессе состязательного обучения. С другой стороны, увеличивая отрицательную логарифмическую плотность реальных записей, можно синтезировать реалистичные поддельные записи таким образом, чтобы они не были слишком близки к реальным записям и снижали вероятность потенциальной утечки информации. Мы провели эксперименты с реальными наборами данных для классификации, регрессии и атак на конфиденциальность. В целом, предлагаемый метод демонстрирует наилучшее качество синтеза (с точки зрения оценочных показателей, ориентированных, например, на задачи F1) при уменьшении отрицательной логарифмической плотности во время состязательного обучения. При увеличении отрицательной логарифмической плотности наши экспериментальные результаты показывают, что расстояние между реальными и поддельными записями увеличивается, повышая устойчивость к атакам на конфиденциальность.</p><h2>1. Введение</h2><p>Генеративные состязательные сети (GAN) и вариационные автокодеры (VAE), получили широкое распространение за последние несколько лет [24, 15, 31, 2, 18, 1, 16]. GAN — одна из наиболее успешных среди генеративных моделей, а синтез табличных данных — одно из многих популярных приложений GAN [7, 4, 28, 27, 21, 38].</p><p>Однако синтез табличных данных является сложной задачей из-за следующих двух проблем:</p><p>1) табличные данные часто содержат конфиденциальную информацию;</p><p>2) требуется делиться табличными данными с людьми, некоторые из которых могут не заслуживать доверия. </p><p>Поэтому создание поддельных записей, максимально похожих на реальные, как это обычно принято для повышения качества синтеза, не всегда предпочтительно при синтезе табличных данных, например, при совместном использовании с непроверенными людьми [27, 5].</p><p>В [5] было показано, что можно эффективно извлекать конфиденциальную информацию из предварительно обученной модели GAN, если ее логарифмические плотности достаточно высоки. С этой целью мы предлагаем обобщенную структуру, называемую <em>обратимым табличным</em> GAN (IT-GAN), в которой мы интегрируем состязательное обучение GAN и обучение с отрицательной логарифмической плотностью обратимых нейронных сетей. В нашей структуре мы можем улучшить или жертвовать отрицательной плотностью логарифмов во время состязательного обучения, чтобы найти компромисс (консенсус) между качеством синтеза и защитой конфиденциальности (см. Рисунок 1).</p><figure class="full-width"><img alt="Рисунок 1: Примеры синтеза с использованием вариантов IT-GAN для переписи, набора данных двоичной классификации. Мы используем [37] для проецирования каждой реальной / поддельной записи на 2-мерное пространство. (а) Синтез только с состязательным обучением показывает разумный результат синтеза. (б) Синтез с состязательным обучением и уменьшением отрицательной логарифмической плотности показывает лучшее сходство между распределениями синих (реальных) и красных (поддельных) точек, чем у (а). (c) Синтез с состязательным обучением а увеличение отрицательной логарифмической плотности улучшает защиту конфиденциальности, т.Е. маловероятно, что синие точки будут правильно выведены из красных точек. Дополнительные цифры приведены в Приложении В." height="263" src="https://habrastorage.org/getpro/habr/upload_files/d9e/484/991/d9e484991a896dc6a834cc7e0a66ba73.PNG" title="Рисунок 1: Примеры синтеза с использованием вариантов IT-GAN для переписи, набора данных двоичной классификации. Мы используем [37] для проецирования каждой реальной / поддельной записи на 2-мерное пространство. (а) Синтез только с состязательным обучением показывает разумный результат синтеза. (б) Синтез с состязательным обучением и уменьшением отрицательной логарифмической плотности показывает лучшее сходство между распределениями синих (реальных) и красных (поддельных) точек, чем у (а). (c) Синтез с состязательным обучением а увеличение отрицательной логарифмической плотности улучшает защиту конфиденциальности, т.Е. маловероятно, что синие точки будут правильно выведены из красных точек. Дополнительные цифры приведены в Приложении В." width="900"/><div><figcaption>Рисунок 1: Примеры синтеза с использованием вариантов IT-GAN для переписи, набора данных двоичной классификации. Мы используем [37] для проецирования каждой реальной / поддельной записи на 2-мерное пространство. (а) Синтез только с состязательным обучением показывает разумный результат синтеза. (б) Синтез с состязательным обучением и уменьшением отрицательной логарифмической плотности показывает лучшее сходство между распределениями синих (реальных) и красных (поддельных) точек, чем у (а). (c) Синтез с состязательным обучением а увеличение отрицательной логарифмической плотности улучшает защиту конфиденциальности, т.Е. маловероятно, что синие точки будут правильно выведены из красных точек. Дополнительные цифры приведены в Приложении В.</figcaption></div></figure><p>Наш генератор, основанный на ансамблях нейронных решений обыкновенных дифференциальных уравнениях (NODE [6]), является обратимым и позволяет реализовать предложенную концепцию обучения. Для NODE существует эффективный метод беспристрастной оценки их определителей Якоби [6, 16]. Таким образом, используя независимую оценку Хатчинсона, мы можем эффективно оценить отрицательную логарифмическую плотность. После этого отрицательная логарифмическая плотность может быть использована для следующих двух противоположных целей:</p><p>1) уменьшение отрицательной логарифмической плотности для дальнейшего повышения качества синтеза (см. IT-GAN(Q) на рисунке 1);</p><p>2) увеличение отрицательной логарифмической плотности для синтеза реалистичных поддельных записей, которые не слишком похожи на реальные записи (см. IT-GAN(L) на рисунке 1). </p><p>В частности, вторая цель, а именно, ухудшение логарифмической плотности при незначительном снижении качества синтеза, тесно связана с проблемой утечки информации в табличных данных.</p><p>Однако у этого обратимого генератора есть одно ограничение – размерность скрытых слоев не может быть изменена. Чтобы преодолеть это ограничение, мы предлагаем интегрированную архитектуру автоэнкодера или автодекодера (AE) и GAN. Мотивация, лежащая в основе предлагаемой совместной архитектуры, двоякая:</p><p>1) роль AE заключается в создании скрытого пространства представления, над которым работают генератор и дискриминатор, а пространство скрытого представления имеет ту же размерность, что и скрытый входной вектор генератора, следовательно, входные и выходные размеры в нашем генераторе одинаковы, что соответствует требованию инвариантности размерности узлов;</p><p>2) разделение труда между АЕ и генератором может улучшить процесс обучения, так как табличные данные обычно содержат большое количество столбцов, что затрудняет синтез. </p><p>В нашей совместной архитектуре генератор разделяет свою задачу с AE; он напрямую синтезирует не поддельные записи, а поддельные скрытые представления. Декодер (сеть восстановления) AE преобразует их в поддельные записи, доступные для чтения человеком. Таким образом, заключительное обучение состоит из обучения GAN, обучения AE и регуляризации отрицательной логарифмической плотности.</p><p>Мы провели эксперименты с 6 реальными табличными наборами данных и сравнили наш метод с 9 базовыми методами. Во многих случаях, оценки результатов по нашим методам превосходят все другие базовые показатели. Наш вклад можно резюмировать следующим образом: </p><p>1. Мы предлагаем общую структуру, в которой можно найти компромисс между качеством синтеза и утечкой информации. </p><p>2. С этой целью мы объединяем состязательное обучение GAN и обучение с отрицательной логарифмической плотностью обратимых нейронных сетей. </p><p>3. Мы проводим тщательные эксперименты с 6 реальными табличными наборами данных, и наши методы превосходят существующие методы почти во всех случаях.</p><h2>2. Литературный обзор (анализ)</h2><p>Мы представляем различные методы синтеза табличных данных и обратимые нейронные сети. В частности, мы рассмотрим обратимую характеристику NODE.</p><h3>2.1. Синтез табличных данных</h3><p>Пусть <strong>X</strong><sub>[1:N]</sub> - табличные данные, состоящие из <em>N</em> столбцов. Каждый столбец <strong>X</strong><sub>[1:N]</sub> является либо дискретным, либо непрерывным и числовым. Пусть x - запись <strong>X</strong><sub>[1:N]</sub>. Цель синтеза табличных данных состоит в том, чтобы:</p><p>1) изучить плотность ˆ<strong>p</strong>(x), которая наилучшим образом аппроксимирует распределение данных <strong>p</strong>(x);</p><p>2) сгенерировать поддельные табличные данные ˆ<strong>X</strong><sub>[1:N]</sub>. </p><p>Для простоты и без потери общности мы предполагаем, что |<strong>X</strong><sub>[1:N]</sub>| = | ˆ<strong>X</strong><sub>[1:N]</sub>| т.е. табличные данные имеют одинаковое количество записей.</p><p>Этот подход может быть реализован с помощью различных подходов, например, VAE, GAN и так далее, и это лишь некоторые из них. Мы представляем несколько оригинальных моделей в этой области. Синтез табличных данных, который генерирует реалистичную синтезированную таблицу путем моделирования совместного распределения вероятностей по столбцам в таблице, включает в себя множество различных методов в зависимости от типов данных. Например, байесовские сети [3, 42] и деревья решений [30] используются для генерации дискретных переменных. Для генерации непрерывных переменных используется рекурсивное моделирование таблиц с использованием гауссовой связки [29]. Для синтеза пространственных данных используется дифференциально-разностный алгоритм декомпозиции [9, 41]. Однако некоторые ограничения, присущие этим моделям, такие как тип распределений и вычислительные проблемы, препятствуют высокоточному синтезу данных.</p><p>В последние годы было внедрено несколько методов генерации данных на основе GAN для синтеза табличных данных, которые в основном обрабатывают непрерывные или дискретные числовые записи. RGAN [13] генерирует непрерывные временные ряды медицинских записей, в то время как  MedGAN   [7], CorrGAN [28] генерируют дискретные записи. А  EhrGAN   [4] генерирует правдоподобные помеченные записи, используя обучение с неполным контролем для увеличения ограниченных обучающих данных. PATE-GAN [21] генерирует синтетические данные, не ставя под угрозу конфиденциальность исходных данных.  TableGAN   [27] улучшил синтез табличных данных с использованием сверточных нейронных сетей, чтобы максимизировать точность прогнозирования по столбцу метки. TGAN [38] - одна из самых последних условных моделей, основанных на GAN. В нем было предложено несколько важных направлений для синтеза табличных данных с высокой точностью, например, механизм предварительной обработки для преобразования каждой записи в форму, подходящую для GAN оптимально.</p><p>2.2. Обратимые нейронные сети и нейронные обыкновенные дифференциальные уравнения</p><p>Обратимые нейронные сети, как правило, являются <em>биективными</em> (взаимно-однозначными). Благодаря этому свойству и теореме об изменении переменной мы можем эффективно рассчитать точную логарифмическую плотность выборки данных <strong>x</strong> следующим образом:</p><figure class="full-width"><img height="64" src="https://habrastorage.org/getpro/habr/upload_files/1e8/81d/1ed/1e881d1ed475c05d6bade70ea40fb3c5.PNG" width="671"/></figure><p>где f : R<sup>dim (z)</sup> → R<sup>dim (x)</sup> - обратимая функция, а <em>z ∼ p<sub>z</sub> (z)</em>.     ∂ f (z) / ∂ z - Якобиан <em>f</em>, который представима эффективно для вычисления — он имеет кубическую временную  сложность. Поэтому обратимые нейронные сети обычно ограничивают определение матрицы Якоби формой, которая может быть эффективно вычислена [31, 36, 25, 26, 10, 11]. Однако FFJORD [16] недавно предложил обратимую архитектуру на основе NODE, в которой мы можем использовать любую форму якобиана. Мы также полагаемся на этот метод.</p><p>В NODE [6] пусть <em>h(t)</em> - скрытый вектор в момент времени (или слой) <em>t</em> в нейронной сети. Узлы решают следующую интегральную задачу для вычисления <em>h(t<sub>i+1</sub>)</em> из h(t<sub>i</sub>) [6]:</p><figure class="full-width"><img height="89" src="https://habrastorage.org/getpro/habr/upload_files/0a5/1dc/583/0a51dc58316a3632fd37ae70b3115a90.PNG" width="666"/></figure><p>где<strong> <em>f</em>(h(t), t; </strong>θf), которую мы называем функцией ODE (обыкновенных дифференциальных уравнений), представляет собой нейронную сеть для аппроксимации </p><figure class="float"><img data-src="https://habrastorage.org/getpro/habr/upload_files/118/98b/aad/11898baad275b2a15eed11c41d8bfefc.png" height="50" src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/118/98b/aad/11898baad275b2a15eed11c41d8bfefc.png" width="124"/></figure><p>Для решения интегральной задачи узлы располагаются в решениях ODE, например, явным методом Эйлера, методом Дормана–Принса и т.д. [12]. <strong><em>h</em></strong><em> (t<sub>i</sub>)</em> легко восстанавливается из <strong><em>h</em></strong><em> (t<sub>i + 1</sub>)</em> с помощью ODE в обратном направлении времени следующим образом:</p><figure class="full-width"><img height="78" src="https://habrastorage.org/getpro/habr/upload_files/8fe/564/61b/8fe56461b402e0e9dafa1fd5c2078d42.PNG" width="664"/></figure><p>Две другие связанные статьи - FlowGAN [17] и TimeGAN [39], хотя они не синтезируют табличные данные. FlowGAN сочетает состязательное обучение с NICE [10] или RealNVP [11]. Гровер и др. в статье показали, что обучение на основе максимального правдоподобия не показывает надежного синтеза для многомерного пространства, и они пытаются объединить их [17]. TimeGAN объединяет, учитывая данные временных рядов, состязательное обучение и контролируемое обучение, вероятности прогнозирования следующего значения по прошлым значениям. Это контролируемое обучение доступно, потому что они имеют дело с данными временных рядов. В нашем случае мы объединяем состязательное обучение и регуляризацию узлов с отрицательной логарифмической плотностью, которые считаются более общими, чем NICE и RealNVP [16].</p><h2>3. Предлагаемый метод</h2><p>Мы предлагаем более продвинутую настройку для синтеза табличных данных, чем существующие методы. Наша цель состоит в том, чтобы интегрировать состязательное обучение GAN и обучение логарифмической плотности обратимых нейронных сетей. Таким образом, можно легко найти компромисс между качеством синтеза и защитой конфиденциальности.</p><p>Опишем в этом разделе наш проект. У него есть два ключевых момента:</p><p>1) мы используем архитектуру обратимой нейронной сети для проектирования нашего генератора;</p><p>2) мы интегрируем  <strong>autoencoder</strong> (AE) в нашу структуру, чтобы <em>a)</em> включить изометрическую архитектуру генератора, т.е. размеры скрытых слоев не меняются, и <em>b)</em> распределять рабочую нагрузку генератора.</p><h3>3.1. Общая архитектура</h3><p>Общая архитектура приведена на рисунке 2. Нашу архитектуру можно разделить на следующие четыре подхода (пути) к передаче данных:</p><figure class="full-width"><img alt="Рисунок 2: Общая архитектура IT-GAN. Каждый цвет края означает определенный тип пути передачи данных." height="344" src="https://habrastorage.org/getpro/habr/upload_files/4f7/161/3c4/4f71613c4d0bd65545d23d41e7fac971.PNG" title="Рисунок 2: Общая архитектура IT-GAN. Каждый цвет края означает определенный тип пути передачи данных." width="647"/><div><figcaption>Рисунок 2: Общая архитектура IT-GAN. Каждый цвет края означает определенный тип пути передачи данных.</figcaption></div></figure><p>1) AE-путь к данным AE, выделенный красным, связан с моделью AE для генерации, учитывая реальную запись <strong>x</strong><sub>real</sub>, скрытое представление h<sub>real</sub> и реконструкцию ˆ<strong>x</strong><sub>real</sub>;</p><p>2) Log density-путь, где существует два разных пути передачи данных, связанных с генератором, а путь логарифмической плотности, выделенный синим цветом, связан с обратимой моделью для вычисления логарифмической плотности скрытого представления h<sub>real</sub>, т.е. log ˆ<strong>p</strong><sub>g</sub>(h<sub>real</sub>), следовательно, мы можем рассматривать это как логарифмическую плотность реальной записи <strong>x</strong><sub>real</sub>, потому что скрытое представление взято из реальной записи;</p><p>3) GAN-путь, путь передачи данных, связанный с генератором, выделенный оранжевым цветом, предназначен для состязательного обучения, а дискриминатор считывает <strong>h</strong><sub>real</sub> и <strong>h</strong><sub>fake</sub>, которые генерируются генератором из скрытого вектора <strong>z</strong>, чтобы различать их;</p><p>4) Cинтез-путь, последний путь к данным, выделенный зеленым цветом, используется после завершения обучения нашей модели и использует генератор и декодер, мы синтезируем множество поддельных записей.</p><h3>3.2. Автоэнкодер (АЕ)</h3><p>Мы описываем нашу модель AE в этом подразделе. Она основана на нормализации, зависящей от режима, которая:</p><p>1) соответствует вариационной смеси гауссианов (функций Гаусса) для каждого непрерывного столбца <strong>X</strong><sub>[1: N]</sub>;</p><p>2) преобразует каждый непрерывный элемент j-й записи <strong>x</strong><sub>j, 1: N</sub> ∈ <strong>X</strong><sub>1: N</sub> в одномерный вектор, обозначающий конкретный гауссиан, который наилучшим образом соответствует элементу и его скалярному нормализованному значению в выбранном гауссиане. </p><p>Если столбец является дискретным, мы просто преобразуем каждое значение в столбце в одномерный вектор. После этого мы используем следующий кодер E: R<sup>dim(</sup><strong><sup>x</sup></strong><sup>)</sup> → R<sup>dim(</sup><strong><sup>h</sup></strong><sup>)</sup> и декодер (сеть восстановления) R: R<sup>dim(</sup><strong><sup>h</sup></strong><sup>)</sup> → R<sup>dim(</sup><strong><sup>x</sup></strong><sup>)</sup>:</p><figure class="full-width"><img height="81" src="https://habrastorage.org/getpro/habr/upload_files/8cb/461/6d2/8cb4616d2eda5fc0f841c692ea78308e.PNG" width="720"/></figure><p>где φ - ReLU, FC1<sub>1</sub> - полностью связующим слоем (Fully Connected - FC), который принимает размер входного сигнала dim(<strong>x</strong>). Благодаря нескольким таким слоям FC1<sub>ne</sub> обеспечивает размер выходного сигнала dim(<strong>h</strong>). Мы используем θe для обозначения параметров кодера. Аналогично, FC2<sub>1</sub> принимает размер dim(<strong>h</strong>) входного сигнала, а после нескольких уровней FC FC2<sub>nr</sub> размер выходного сигнала становится равным dim(<strong>x</strong>). Мы используем <strong><em>θ<sub>r</sub></em></strong> для обозначения параметров декодера (сети восстановления).</p><h3>3.3. Генератор</h3><p>Ключевым моментом в нашей конструкции генератора является использование обратимой нейронной сети [16] для наших собственных целей. Среди различных обратимых архитектур мы внедряем и настраиваем узлы. Обратимые модели, основанные на NODE, имеют то преимущество, что нет ограничений на форму определителя Якоби. Другие обратимые модели обычно ограничивают свои якобианы конкретными формами, которые можно легко вычислить. Однако наша модель, основанная на N, не имеет таких ограничений.</p><p>Таким образом, мы могли бы изучить архитектуру генератора без каких-либо ограничений и, наконец, использовать следующую архитектуру для нашего генератора G: R<sup>dim(</sup><strong><sup>z</sup></strong><sup>)</sup> → R<sup>dim(</sup><strong><sup>h</sup></strong><sup>)</sup>:</p><figure class="full-width"><img height="171" src="https://habrastorage.org/getpro/habr/upload_files/9e5/483/56d/9e548356dcdcd33d897efa0df3afacfa.PNG" width="755"/></figure><p>где σ - нелинейная активация, FC(<sub>1,1</sub>) и FC(<sub>1,2</sub>) - с (dim(<strong>z</strong>), M dim(<strong>z</strong>)), FC(<sub>K,1</sub>) и FC(<sub>K,2</sub>) - с (M dim(<strong>z</strong>), dim(<strong>h</strong>)), а FC(<sub>i,1</sub>) и FC(<sub>i,2</sub>) - с (M dim(<strong>z</strong>), M dim(<strong>z</strong>)), если 1 &lt; <em>i</em> &lt; K.  <strong><em>z</em></strong>(0) ∼<strong><em>N</em></strong>(0,1) - скрытый вектор, выбранный из единичного гауссова. M<sub>i</sub>(z,t) : R<sup>dim(z)</sup> → [0, 1] - это функция отображения, которая определяет пропорцию между FC<sub>(i,1)</sub> и FC<sub>(i,2)</sub>. Мы используем либо t, либо sigmoid (сигмоидальную форму) (FC<sub>MI</sub> (z ⊕ t)) для M<sub>i</sub>, где ⊕ означает конкатенацию (соединение). В нашем случае dim(<strong>z</strong>) = dim(<strong>h</strong>). Фактически, эта инвариантная размерность является одной из характеристик многих обратимых нейронных сетей. Мы используем θ<sub>g</sub> для обозначения параметров генератора.</p><p>Интегральная задача может быть решена различными решателями ODE. Логарифмическая вероятность log p (<strong><em>h</em></strong><sub>fake</sub>) может быть рассчитана с помощью независимой оценки Хатчинсона следующим образом:</p><figure class="full-width"><img height="80" src="https://habrastorage.org/getpro/habr/upload_files/7f3/507/7d6/7f35077d6d3cba0fc4895c35269ca4f4.PNG" width="735"/></figure><p>где <strong><em>p(<sub>E</sub>)</em></strong> - стандартное распределение Гаусса или Радемахера [19]. Временная сложность вычисления оценки Хатчинсона немного больше, чем при оценке <strong><em>f</em></strong>, поскольку векторное произведение якобианов</p><figure class="float"><img data-src="https://habrastorage.org/getpro/habr/upload_files/953/3fe/d1f/9533fed1f657e785cbfea95a4d43cb58.png" height="56" src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/953/3fe/d1f/9533fed1f657e785cbfea95a4d43cb58.png" width="101"/></figure><p> имеет ту же стоимость, что и при оценке <strong><em>f</em></strong> с использованием автоматического дифференцирования в обратном режиме.</p><p>Одним из отличительных свойств генератора является то, что при наличии реального скрытого вектора hreal мы можем восстановить </p><figure class="float"><img data-src="https://habrastorage.org/getpro/habr/upload_files/9b2/bb7/cd7/9b2bb7cd70d3b18012e9c3c7c5bb98b9.png" height="43" src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/9b2/bb7/cd7/9b2bb7cd70d3b18012e9c3c7c5bb98b9.png" width="293"/></figure><p> и оценить его логарифмическую вероятность <em>ˆp(ˆ</em><strong><em>h</em></strong><em><sub>real</sub>)</em>, где <strong><em>h</em></strong><em><sub>real</sub> = ˆ</em><strong><em>h</em></strong><em><sub>real</sub>,</em> точно решив уравнение. (3) — обратите внимание, что G<sup>−1</sup> аналитически определяется из G и не требует обучения.</p><h3>3.4. Дискриминатор</h3><p>Дискриминатор D : <em>R</em><sup>dim(h)</sup> → <em>R</em> считывает h<sub>real</sub> и h<sub>fake</sub> для их классификации. Мы используем следующую архитектуру:</p><figure class="full-width"><img height="64" src="https://habrastorage.org/getpro/habr/upload_files/f82/18c/c45/f8218cc45cad4904c55c29c07df5ec7a.PNG" width="644"/></figure><p>где <strong>FC3<sub>1</sub></strong> принимает размер dim(<strong>h</strong>) входного сигнала, и после нескольких слоев FC FC3<em><sub>nd</sub></em> создает одномерный выходной сигнал. <em>n<sub>b</sub></em> - протекающий ReLU с отрицательным наклоном b, а Φ<sub>a</sub> - dropout, отсев с коэффициентом a (распространения ошибок, уменьшения взаимодействий узлов слоя). Мы используем <em>θ<sub>d</sub></em> для обозначения параметров дискриминатора.</p><h3>3.5. Алгоритм Обучения</h3><p>Мы опишем, как обучить предлагаемую архитектуру. Поскольку он состоит из нескольких модулей и их функций потери (отклонения), мы сначала отдельно опишем их, а затем окончательный алгоритм обучения. </p><p>Функции потерь. Мы вводим различные функции потерь, которые используем для обучения нашей модели. Во-первых, мы используем следующие потери AE для обучения кодера и модели декодера:</p><figure class="full-width"><img height="72" src="https://habrastorage.org/getpro/habr/upload_files/c21/2a4/7ac/c212a47ac447b20c64d709ecdecd1773.PNG" width="746"/></figure><p>где <em>L<sub>Reconstruct</sub></em> – стандартное отклонение ошибки восстановления. </p><p>Отметим, что здесь мы хотим изучить разреженный кодировщик согласно регуляризации L<sup>2</sup>. Вектор h<sub>fake</sub> - скрытый вектор, сгенерированный генератором G. Вектор ˆh<sub>fake</sub> - восстановленный с помощью E(R(<strong>h</strong><sub>fake</sub>)) скрытый вектор, где E и R означают кодер и декодер соответственно. После некоторых предварительных исследований мы обнаружили, что это определение потерь обеспечивает надежную подготовку во многих случаях. В частности, это дополнительно стабилизирует устойчивость автоэнкодера на изучаемом пространстве скрытого представления. Для обучения генератора и дискриминатора мы используем WGAN-GP loss. Затем мы предлагаем использовать следующий регуляризатор для управления логарифмической плотностью, что приводит к корректировке расстояния между реальным и поддельным:</p><figure class="full-width"><img height="70" src="https://habrastorage.org/getpro/habr/upload_files/30c/204/29d/30c20429dce8059ba5c1c9b6107dcef6.PNG" width="676"/></figure><p>где E - кодировщик, γ − коэффициент, подчеркивающий регуляризацию, а логарифмическая плотность log <strong><em>ˆp </em></strong>(E(<strong>x</strong>)) может быть вычислена с помощью уравнения (7) во время              <strong>G</strong>(<strong>G</strong><sup>-1</sup>(E(<strong><em>x</em></strong>))).</p><p>Алгоритм обучения. Алгоритм 1 описывает наш метод обучения. Существует обучающий набор данных <em>D <sub>train</sub></em>. Сначала мы обучаем кодер потерям AE и WGAN-GP, а декодер потерям AE (строка 4). Чтобы изучить скрытое векторное пространство, подходящее для общего процесса синтеза, мы обучаем кодировщик потерям WGAN-GP, чтобы помочь дискриминатору лучше различать реальные и поддельные скрытые векторы по влиянию скрытого вектора на дискриминатор. Делая это, AE и GAN интегрируются в единую структуру. Затем мы обучаем дискриминатор с потерей WGAN-GP каждый <em>период <sub>D</sub></em> (строка 6), генерируя потери WGAN-GP каждый <em>период <sub>G</sub></em> (строка 9). После этого генератор обучается еще раз с помощью предлагаемого регуляризатора плотности каждый <em>период <sub>L</sub></em> (строка 12). Поскольку дискриминатор и генератор основаны на скрытом векторе, созданном моделью AE, мы затем обучаем кодер и декодер на каждой итерации. Регуляризация логарифмической плотности также используется не всегда, а на каждой итерации <em>периода <sub>L</sub></em>, потому что мы обнаружили, что частая регуляризация логарифмической плотности отрицательно влияет на весь прогресс обучения. Используя данные оценки <em>D <sub>val</sub></em> и метрику оценки, ориентированную на конкретные задачи, мы выбираем наилучшую модель. Например, мы используем оценку F-1 / MSE обученного генератора каждый период с проверяющими данными — период состоит из множества итераций в зависимости от количества записей в обучающих данных и размера выборки. Если недавняя модель лучше, чем текущая лучшая модель, мы ее обновляем.</p><figure class=""><img height="546" src="https://habrastorage.org/getpro/habr/upload_files/664/2a2/50e/6642a250e1a947796da325b044fd0829.PNG" width="470"/></figure><p>О податливости обучения генератора. Версия теоремы Коши–Ковалевского для ODE гласит, что для заданного <em>f</em> = <sup>dh (t)</sup> / <sub>dt</sub> существует единственное решение <strong>h</strong>, если <strong><em>f</em></strong> является аналитическим (непрерывным в малом по Липшицу). Другими словами, задача ODE корректна, если <em>f</em> аналитическая [14]. В нашем случае функция <em>f</em> в уравнении (6) использует различные уровни FC, которые являются аналитическими, и некоторые нелинейные случаи, которые могут быть или не быть аналитическими. Однако в наших экспериментах в основном используется гиперболический тангенс (tanh), который является аналитическим. Это означает, что для генератора будет только единственное оптимальное ODE, заданное скрытым вектором z. Из-за уникальности решения и наших относительно более простых определений по сравнению с другими приложениями NODE, например, сверточного слоя, за которым следует ReLU в [6], мы считаем, что наш метод обучения может найти хорошее решение для генератора.</p><h2>4. Экспериментальные оценки</h2><p>Мы представляем наши экспериментальные среды и результаты для синтеза табличных данных. Эксперименты проводились в следующих программных и аппаратных средах: UBUNTU 18.04, PYTHON 3.7.7, NUMPY 1.19.1, SCIPY 1.5.2, PYTORCH 1.8.1, CUDA 11.2 и драйвер NVIDIA 417.22, процессор i9 и NVIDIA RTX TITAN.</p><figure class="full-width"><img height="535" src="https://habrastorage.org/getpro/habr/upload_files/038/6b9/f49/0386b9f498cc2ceb4542acd0517b0b1f.PNG" width="836"/></figure><h3>4.1. Экспериментальные среды</h3><h4>4.1.1 Наборы данных</h4><p>Мы тестируем с различными реальными табличными данными, ориентируясь на двоичную / много-классовую классификацию и регрессию. Их статистические данные обобщены в приложении A. Список наборов данных выглядит следующим образом: Adult (<em>Взрослые</em>) [32] состоит из разнообразной демографической информации в США, извлеченной из переписи населения 1994 года, где мы прогнозируем два класса доходов с высоким (&gt;50 тыс. долл.) и низким (≤50 тыс. долл.) доходом. Census(<em>Перепись населения</em>) [33] похожа на перепись взрослого населения, но в ней есть другие столбцы. Credit (<em>Кредит</em>) [40] предназначен для прогнозирования статуса банковского кредита. Cabs (<em>Такси</em>) [34] собирается индийской сервисной компанией-агрегатором такси для прогнозирования типов клиентов. King (<em>Король</em>) [23] содержит цены продажи домов в округе Кинг в Сиэтле за период с мая 2014 по май 2015 года. News (<em>Новости</em>) [22] имеют разнородный набор функций о статьях, опубликованных Mashable в течение двух лет, для прогнозирования количества акций в социальных сетях. Adult и Перепись предназначены для двоичной классификации, а кредит и такси - для много-классовой классификации. Остальные предназначены для регрессии.</p><h4>4.1.2 Методы оценки</h4><p>Мы генерируем поддельные табличные данные и обучаем алгоритмы множественной классификации (SVM, DecisionTree, AdaBoost и MLP) или регрессии (линейная регрессия и MLP). Затем мы оцениваем их с помощью данных тестирования и усредняем их производительность с точки зрения различных показателей оценки. Этот конкретный метод оценки был предложен в [38], и мы строго следуем их процедуре оценки. Мы выполняем эти процедуры пять раз с пятью разными начальными номерами.</p><p>Для нашего IT-GAN мы рассматриваем IT-GAN (Q) с положительным γ, который уменьшает отрицательную логарифмическую плотность для улучшения <strong><u>Q</u></strong>uality(качества) синтеза, IT-GAN (L), который жертвует логарифмической плотностью для уменьшения <strong><u>L</u></strong>eakage с отрицательным γ, и IT-GAN, который не использует регуляризацию логарифмической плотности. В наших таблицах результатов Real означает, что мы используем реальные табличные данные для обучения модели классификации/регрессии. Для получения других исходных данных обратитесь к Приложению C.</p><p>Мы не сообщаем о некоторых исходных показателях в наших таблицах результатов, чтобы сэкономить места, если их результаты значительно хуже, чем у других. Мы ссылаемся на Приложение E для получения их полных таблиц результатов.</p><figure class="full-width"><img height="283" src="https://habrastorage.org/getpro/habr/upload_files/e8d/d5e/658/e8dd5e658bed59ffb3c738235c3ae027.PNG" width="926"/></figure><h4>4.1.3. Гиперпараметры</h4><p>Мы рассматриваем следующие диапазоны гипер-параметров: номера слоев в кодере и декодере, <em>n<sub>e</sub></em> и <em>n<sub>r</sub></em>, находятся в {2, 3}. Количество слоев <em>n<sub>d</sub></em> эквалайзера (8) находится в {2, 3}. Коэффициент отсева a равен {0, 0.5}, а наклон утечки <em>b</em> равен {0, 0.2}. Нелинейная активация <em>σ</em> равна <em>tanh</em>; коэффициент усиления M уравнения (6) находится в {1, 1.5}; количество слоев K уравнения (6) равен 3; коэффициент <strong><em>γ</em></strong> находится в {-0.1, -0.014, -0.012, -0.01, 0, 0.01, 0.014, 0.05, 0.1}; периоды обучения, обозначенные <em>period<sub>D</sub></em>, <em>period<sub>G</sub></em>, <em>period<sub>L</sub></em> в алгоритме 1 находятся в {1, 3, 5, 6}; размерность скрытого вектора dim(<strong><em>h</em></strong>) равна {32, 64, 128}; размер мини-пакета равен 2000. Мы используем метод обучения/проверки в алгоритме 1. В качестве исходных условий мы рассматриваем рекомендуемый набор гипер-параметров в их статьях или в их уважаемых репозиториях GitHub. Обратитесь к Приложению D для получения наилучших наборов гипер-параметров.</p><h3>4.2. Результаты экспериментов</h3><p> В таблицах результатов лучшие (соответственно, следующие за ними) результаты выделены жирным шрифтом (соответственно с подчеркиванием). Если среднее значение такое же, выигрывает вариант с меньшим стандартным отклонением (std. dev).  В 17 из 18 случаев (# <em>наборы данных</em> × # <em>показатели оценки, ориентированные на задачи</em>) один из наших методов показывает наилучшую производительность у <em>Adult</em>.</p><p>Двоичная классификация. Мы описываем экспериментальные результаты взрослого населения и переписи населения в таблицах 1 и 2. Они представляют собой наборы данных двоичной классификации. В целом, многие методы показывают разумные оценочные баллы, за исключением Ind, Uniform и VeeGAN. В то время как TGAN и TVAE показывают разумную производительность с точки зрения F1 и ROCAUC для <em>Adult(взрослых)</em>, наш метод IT-GAN(Q) показывает наилучшую производительность в целом. Среди этих базовых показателей TGAN демонстрирует хорошую производительность.</p><p>Для <em>Census (</em>переписи населения) TVAE по-прежнему работает хорошо. В то время как TGAN показывает хорошие показатели для взрослого населения, он не показывает разумных показателей в переписи. Наш метод превосходит их. В целом, IT-GAN(Q) является лучшим в <em>Census (переписи)</em>.</p><p>Много-классовая классификация (таксономия). Credit (кредиты) и Taxi (такси) - это области много-классовых классификационных наборов данных, для которых сложно синтезировать. Для оценки в таблице 3 IT-GAN(Q) показывает наилучшие показатели с точки зрения Макро-F1 и ROCAUC, а TVAE показывает наилучший показатель Микро-F1. Это вызвано проблемой различий классов, когда минимальный класс занимает часть в 9%. TVAE не создает для него никаких рекордов и достигает лучшего результата Micro F1. Следовательно, IT-GAN(Q) является лучшим в зачете. На рисунке 4 в приложении B IT-GAN(L) активно синтезирует поддельные записи, которые не перекрываются с реальными записями, что приводит к неоптимальным результатам.</p><p>Для <em>такси(Cabs)</em> в таблице 4 IT-GAN(Q) показывает лучшие результаты с точки зрения микро/макро F1. Интересно, что IT-GAN(L) имеет вторые лучшие результаты. Исходя из этого, мы можем знать, что погрешности, вызванные увеличением отрицательной логарифмической плотности, в этом наборе данных не слишком велики.</p><p>Регрессия. <em>King и News</em> - это регрессионные наборы данных. Многие методы демонстрируют плохие качества в этих наборах данных и задачах, поэтому мы удалили их из таблиц 5 и 6. В частности, все методы, кроме TGAN, IT-GAN и его вариаций, показывают отрицательные оценки линейной регрессии R<sup>2</sup>  в News. Только наши методы показывают надежные результаты синтеза по всем показателям. Для <em>King</em> наш метод и TGAN показывают хорошую производительность, но IT-GAN(Q) превосходит все базовые показатели почти по всем показателям.</p><p>Атака на конфиденциальные данные. В [5] был предложен полный метод атаки на конфиденциальность типа «черный ящик» для GAN. Мы внедрили их метод для атаки на наш метод и измерили показатель успешности атаки в терминах ROCAUC. В таблице 7 мы приводим полные оценки успешности атаки черного ящика только для нашего метода. Более подробные результаты приведены в Приложении G. В большинстве случаев IT-GAN(L) показывает самый низкий показатель успешности атаки. Этот конкретный IT-GAN(L) - это тот, который мы использовали для отчета о производительности в других таблицах.</p><figure class="full-width"><img alt="Рисунок 3: p.d.f. реального и поддельного расстояния" height="234" src="https://habrastorage.org/getpro/habr/upload_files/6f7/d3e/481/6f7d3e481295dfa13f732f73268183e5.PNG" title="Рисунок 3: p.d.f. реального и поддельного расстояния" width="950"/><div><figcaption>Рисунок 3: p.d.f. реального и поддельного расстояния</figcaption></div></figure><p>Исследование абляции при отрицательной логарифмической плотности. Мы сравниваем IT-GAN и его вариации. IT-GAN (Q) превосходит IT-GAN по показателям "Adult (<em>Взрослый)</em>", "<em>Census(Перепись)</em>", "<em>Credit (Кредит)</em>", "<em>Cabs(Такси)</em>" и "<em>King(Король)</em>". В соответствии с этим мы можем сделать вывод, что уменьшение отрицательной логарифмической плотности улучшает показатели оценки, ориентированные на задачи.</p><figure class="full-width"><img height="430" src="https://habrastorage.org/getpro/habr/upload_files/42c/adb/018/42cadb01889aaf4da6b58272812626dc.PNG" width="933"/></figure><p>На рис. 3 мы показываем функцию плотности наборов данных, не зависящих от расстояния в реальном времени, где их средние значения показаны в других таблицах результатов эксперимента. IT-GAN(L) эффективно упорядочивает по расстоянию (радиусу таксона). IT-GAN (Q) показывает больше похожих распределений, чем IT-GAN. Следовательно, мы можем знать, что управление отрицательной логарифмической плотностью работает так, как задумано. Визуализация на рис. 1 также доказывает это.</p><p>Анализ чувствительности. Изменяя два ключевых гипер-параметра dim(<strong><em>h</em></strong>) и <strong><em>γ</em></strong> в наших методах, мы также проводим анализ чувствительности. Мы тестируем IT-GAN(L) с различными настройками для dim(<strong><em>h</em></strong>). В целом, dim(<strong><em>h</em></strong>)=128 дает наилучший результат, как показано в таблице 8. С помощью IT-GAN в таблице 9 мы изменяем <strong>γ</strong>, и <strong>γ</strong> = 0,01 дает много хороших результатов. В таблице 10 <strong>γ</strong>=-0,011 устойчив к полной атаке черного ящика. Другие таблицы приведены в Приложении F.</p><h2>5. Выводы</h2><p>Мы решили проблему синтеза табличных данных с помощью состязательного обучения GAN и регуляризации с отрицательной логарифмической плотностью обратимых нейронных сетей. Наши экспериментальные результаты показывают, что предлагаемые методы хорошо работают в большинстве случаев, а регуляризация с отрицательной логарифмической плотностью может скорректировать компромисс между качеством синтеза и устойчивостью к атаке на конфиденциальность. Однако мы обнаружили, что некоторые наборы данных сложно синтезировать, т.е. все генерируемые модели показывают более низкую производительность, чем реальная, в некоторых много-классовых и/или несбалансированных наборах данных, например, для переписи и кредита. Кроме того, наиболее эффективный метод варьируется от одного набора данных/задачи к другой, и для них все еще существует возможность улучшить свои качества.</p><p>Социальные последствия и ограничения. Наши исследования будут способствовать более активному обмену и публикации табличных данных. Можно использовать наш метод для синтеза поддельных данных, но неясно, как противник может извлечь выгоду из наших исследований. В то же время, однако, существуют возможности для повышения качества синтеза табличных данных. Все еще недостаточно изучено, можно ли использовать поддельные табличные данные для общих задач машинного обучения (хотя мы показали, что их можно использовать для классификации и регрессии).</p><p>Набор данных. Мы суммируем статистику наших наборов данных следующим образом: </p><p>1. Adult ("Взрослый") содержит 22 тыс. обучающих, 10 тыс. тестовых записей с 6 непрерывными числовыми, 8 категориальными и 1 дискретными числовыми столбцами. </p><p>2.Census ("Перепись") содержит 200 тыс. обучающих, 100 тыс. тестовых записей с 7 непрерывными числовыми, 34 категориальными и 0 дискретными числовыми столбцами. </p><p>3. Credit ("Кредит") содержит 30 тысяч записей об обучении, 5 тысяч записей о тестировании с 13 непрерывными числовыми, 4 категориальными и 0 дискретными числовыми столбцами. </p><p>4. Cabs ("Такси") содержит 40 тыс. записей об обучении, 5 тыс. записей о тестировании с 8 непрерывными числовыми, 5 категориальными и 0 дискретными числовыми столбцами. </p><p>5. King ("Король") имеет 16 тысяч записей об обучении, 5 тысяч записей о тестировании с 14 непрерывными числовыми, 2 категориальными и 0 дискретными числовыми столбцами. </p><p>6.News ("Новости") содержат 32 тысячи записей об обучении, 8 тысяч записей о тестировании с 45 непрерывными числовыми, 14 категориальными и 0 дискретными числовыми столбцами.</p><hr/><details class="spoiler"><summary>Ссылки</summary><div class="spoiler__content"><p>[1] Adler, J. and Lunz, S. Banach wasserstein gan. In NeurIPS. 2018.<br/> [2] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In<br/> ICML, 2017.<br/> [3] Aviñó, L., Ruffini, M., and Gavaldà, R. Generating synthetic but plausible healthcare record<br/> datasets, 2018.<br/> [4] Che, Z., Cheng, Y., Zhai, S., Sun, Z., and Liu, Y. Boosting deep learning risk prediction with<br/> generative adversarial networks for electronic health records. 2017.<br/> [5] Chen, D., Yu, N., Zhang, Y., and Fritz, M. Gan-leaks: A taxonomy of membership inference<br/> attacks against generative models. In CCS, 2020.<br/> [6] Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential<br/> equations. In NeurIPS. 2018.<br/> [7] Choi, E., Biswal, S., Maline, A. B., Duke, J., Stewart, F. W., and Sun, J. Generating multi-label<br/> discrete electronic health records using generative adversarial networks. 2017.<br/> [8] Chow, C. and Liu, C. Approximating discrete probability distributions with dependence trees.<br/> IEEE Transactions on Information Theory, 14(3):462–467, 1968.<br/> [9] Cormode, G., Procopiuc, M., Shen, E., Srivastava, D., and Yu, T. Differentially private spatial<br/> decompositions. 2011.<br/> [10] Dinh, L., Krueger, D., and Bengio, Y. NICE: non-linear independent components estimation. In<br/> ICLR, 2015.<br/> [11] Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using real NVP. In ICLR, 2017.<br/> [12] Dormand, J. and Prince, P. A family of embedded runge-kutta formulae. Journal of Computational and Applied Mathematics, 6(1):19 – 26, 1980.<br/> [13] Esteban, C., Hyland, L. S., and Rätsch, G. Real-valued (medical) time series generation with<br/> recurrent conditional gans, 2017.<br/> [14] FOLLAND, G. B. Introduction to Partial Differential Equations: Second Edition, volume 102.<br/> Princeton University Press, 1995.<br/> [15] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,<br/> and Bengio, Y. Generative adversarial nets. In NeurIPS. 2014.<br/> [16] Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., and Duvenaud, D. Ffjord:<br/> Free-form continuous dynamics for scalable reversible generative models. In ICLR, 2019.<br/> [17] Grover, A., Dhar, M., and Ermon, S. Flow-gan: Combining maximum likelihood and adversarial<br/> learning in generative models. In AAAI, 2018.<br/> [18] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. Improved training of<br/> wasserstein gans. In NeurIPS, 2017.<br/> [19] Hutchinson, M. A stochastic estimator of the trace of the influence matrix for laplacian<br/> smoothing splines. Communications in Statistics - Simulation and Computation, 19(2), 1990.<br/> [20] Ishfaq, H., Hoogi, A., and Rubin, D. Tvae: Triplet-based variational autoencoder using metric<br/> learning, 2018.<br/> [21] Jordon, J., Yoon, J., and Schaar, V. D. M. Pate-gan: Generating synthetic data with differential<br/> privacy guarantees. In International Conference on Learning Representations, 2019.<br/> [22] Kelwin Fernandes. Online News Popularity Data Set. <a href="https://archive.ics.uci.edu/ml/" rel="noopener noreferrer nofollow">https://archive.ics.uci.edu/ml/</a><br/> datasets/Twenty+Newsgroups, 2015.<br/> [23] King County. House Sales in King County, USA. <a href="https://www.kaggle.com/harlfoxem/" rel="noopener noreferrer nofollow">https://www.kaggle.com/harlfoxem/</a><br/> housesalesprediction, 2021.<br/> [24] Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In ICLR, 2014.<br/> [25] Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. Improved<br/> variational inference with inverse autoregressive flow. In NeurIPS, 2016.<br/> [26] Oliva, J., Dubey, A., Zaheer, M., Poczos, B., Salakhutdinov, R., Xing, E., and Schneider, J.<br/> Transformation autoregressive networks. In ICML, 2018.<br/> [27] Park, N., Mohammadi, M., Gorde, K., Jajodia, S., Park, H., and Kim, Y. Data synthesis based<br/> on generative adversarial networks. 2018.<br/> [28] Patel, S., Kakadiya, A., Mehta, M., Derasari, R., Patel, R., and Gandhi, R. Correlated discrete<br/> data generation using adversarial training. 2018.<br/> [29] Patki, N., Wedge, R., and Veeramachaneni, K. The synthetic data vault. In DSAA, 2016.<br/> [30] Reiter, P. J. Using cart to generate partially synthetic, public use microdata. Journal of Official<br/> Statistics, 21:441, 01 2005.<br/> [31] Rezende, D. and Mohamed, S. Variational inference with normalizing flows. volume 37 of<br/> Proceedings of Machine Learning Research, pp. 1530–1538, 2015.<br/> [32] Ronny Kohavi. Adult Data Set. <a href="http://archive.ics.uci.edu/ml/datasets/adult" rel="noopener noreferrer nofollow">http://archive.ics.uci.edu/ml/datasets/adult</a>,<br/> 1996.<br/> [33] Ronny Kohavi. Census Income Data Set. <a href="https://archive.ics.uci.edu/ml/datasets/" rel="noopener noreferrer nofollow">https://archive.ics.uci.edu/ml/datasets/</a><br/> census+income, 1996.<br/> [34] Sigma Cabs. Trip Pricing with Taxi Mobility Analytics. <a href="https://www.kaggle.com/" rel="noopener noreferrer nofollow">https://www.kaggle.com/</a><br/> arashnic/taxi-pricing-with-mobility-analytics, 2021.<br/> [35] Srivastava, A., Valkov, L., Russell, C., Gutmann, M. U., and Sutton, C. Veegan: Reducing mode<br/> collapse in gans using implicit variational learning. In NeurIPS. 2017.<br/> [36] van den Berg, R., Hasenclever, L., Tomczak, J., and Welling, M. Sylvester normalizing flows<br/> for variational inference. In UAI, 2018.<br/> [37] van der Maaten, L. and Hinton, G. Visualizing data using t-sne. Journal of Machine Learning<br/> Research, 9(86), 2008.<br/> [38] Xu, L., Skoularidou, M., Cuesta-Infante, A., and Veeramachaneni, K. Modeling tabular data<br/> using conditional gan. In NeurIPS. 2019.<br/> [39] Yoon, J., Jarrett, D., and van der Schaar, M. Time-series generative adversarial networks. In<br/> NeurIPS, 2019.<br/> [40] Zaur Begiev. Bank Loan Status Dataset. <a href="https://www.kaggle.com/zaurbegiev/" rel="noopener noreferrer nofollow">https://www.kaggle.com/zaurbegiev/</a><br/> my-dataset?select=credit_train.csv, 2017.<br/> [41] Zhang, J., Xiao, X., and Xie, X. Privtree: A differentially private algorithm for hierarchical<br/> decompositions. 2016.<br/> [42] Zhang, J., Cormode, G., Procopiuc, C. M., Srivastava, D., and Xiao, X. Privbayes: Private data<br/> release via bayesian networks. ACM Transactions on Database Systems, 2017.</p></div></details><p></p></div></div></div> <!-- --> <!-- --></div> <!-- --> <!-- --></div> <!-- --> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a class="tm-tags-list__link" href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bgan%5D">gan</a></li><li class="tm-separated-list__item"><a class="tm-tags-list__link" href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bneural%20networks%5D">neural networks</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a class="tm-hubs-list__link" href="/ru/hub/algorithms/">Алгоритмы</a></li><li class="tm-separated-list__item"><a class="tm-hubs-list__link" href="/ru/hub/machine_learning/">Машинное обучение</a></li></ul></div></div></article></div> <!-- --></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon tm-votes-meter__icon_appearance-article" height="24" width="24"><title>Всего голосов 2: ↑1 и ↓1</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-rating"></use></svg> <span class="tm-votes-meter__value tm-votes-meter__value tm-votes-meter__value_appearance-article tm-votes-meter__value_rating" title="Всего голосов 2: ↑1 и ↓1">0</span></div> <div class="v-portal" style="display:none;"></div></div> <!-- --> <!-- --> <button class="bookmarks-button tm-data-icons__item" title="Добавить в закладки" type="button"><span class="tm-svg-icon__wrapper bookmarks-button__icon"><svg class="tm-svg-img tm-svg-icon" height="24" width="24"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-favorite"></use></svg></span> <span class="bookmarks-button__counter" title="Количество пользователей, добавивших публикацию в закладки">
    17
  </span></button> <div class="tm-sharing tm-data-icons__item" title="Поделиться"><button class="tm-sharing__button" type="button"><svg class="tm-sharing__icon" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13.8 13.8V18l7.2-6.6L13.8 5v3.9C5 8.9 3 18.6 3 18.6c2.5-4.4 6-4.8 10.8-4.8z" fill="currentColor"></path></svg></button> <div class="v-portal" style="display:none;"></div></div> <div class="tm-article-comments-counter-link tm-data-icons__item" title="Читать комментарии"><a class="tm-article-comments-counter-link__link" href="/ru/post/668146/comments/"><svg class="tm-svg-img tm-article-comments-counter-link__icon" height="24" width="24"><title>Комментарии</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value">
      0
    </span></a> <!-- --></div> <!-- --> <div class="v-portal" style="display:none;"></div></div></div></div> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!-- --> <div></div> <section class="tm-block tm-block tm-block_spacing-bottom"><!-- --> <div class="tm-block__body tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"> <div class="tm-user-card tm-article-author__user-card tm-user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a class="tm-user-card__userpic tm-user-card__userpic_size-40" href="/ru/users/AyratGil/"><div class="tm-entity-image"><svg class="tm-svg-img tm-image-placeholder tm-image-placeholder_pink"><!-- --> <use xlink:href="/img/megazord-v28.617e16ca..svg#placeholder-user"></use></svg></div></a> <div class="tm-user-card__meta"><div class="tm-counter-container tm-karma tm-karma" title=" 11 голосов "><div class="tm-counter-container__header"><div class="tm-karma__votes tm-karma__votes_positive">
      7
    </div></div> <div class="tm-counter-container__footer"><div class="tm-karma__text">
      Карма
    </div> <div class="v-portal" style="display:none;"></div></div></div> <div class="tm-counter-container" title="Рейтинг пользователя"><div class="tm-counter-container__header"> <div class="tm-votes-lever tm-votes-lever tm-votes-lever_appearance-rating"><!-- --> <div class="tm-votes-lever__score tm-votes-lever__score tm-votes-lever__score_appearance-rating"><span class="tm-votes-lever__score-counter tm-votes-lever__score-counter tm-votes-lever__score-counter_rating">
        0
      </span></div> <!-- --></div></div> <div class="tm-counter-container__footer"><span class="tm-rating__text tm-rating__text">
      Рейтинг
    </span></div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title tm-user-card__title_variant-article"><!-- --> <a class="tm-user-card__nickname tm-user-card__nickname tm-user-card__nickname_variant-article" href="/ru/users/AyratGil/">
          @AyratGil
        </a> <!-- --></div> <p class="tm-user-card__short-info tm-user-card__short-info tm-user-card__short-info_variant-article">Аналитик</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons tm-user-card__buttons_variant-article"><!-- --> <!-- --> <!-- --> <!-- --> <!-- --></div></div> <!-- --></div> <div class="v-portal" style="display:none;"></div></div> <!-- --></section> <div class="tm-adfox-banner__container tm-page-article__banner"><!-- --> <div class="tm-adfox-banner tm-adfox-banner tm-adfox-banner_variant-leaderboard" id="adfox_164725660339535756"></div></div> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style" href="/ru/post/668146/comments/"><svg class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted" height="24" width="24"><title>Комментарии</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментировать 
    </span></a> <!-- --></div></div></div> <section class="tm-block tm-block tm-block_spacing-bottom"><header class="tm-block__header tm-block__header tm-block__header_variant-borderless"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title tm-block__title_variant-large">Публикации</h2> </div> <!-- --></header> <div class="tm-block__body tm-block__body tm-block__body_variant-condensed-slim"><div class="tm-tabs tm-tabs"><div><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link tm-tabs__tab-link_active tm-tabs__tab-link_slim">
        Лучшие за сутки
      </button></span><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link tm-tabs__tab-link_slim">
        Похожие
      </button></span></div> <!-- --></div> <div class="similar-and-daily__tab-view"><div><!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <div class="tm-placeholder-article-cards"><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div></div> <!-- --> <!-- --> <!-- --> <!-- --></div> <!-- --></div></div> <!-- --></section> <section class="tm-block tm-stories-block tm-block tm-block_spacing-around" data-async-called="true" data-navigatable="" tabindex="0"><header class="tm-block__header tm-block__header"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title">Истории</h2> </div> <!-- --></header> <div class="tm-block__body tm-block__body tm-block__body_variant-equal"><div class="tm-stories-empty"><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div></div> <!-- --></div> <!-- --></section> <div><!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <div class="tm-placeholder-inset tm-placeholder-vacancies"><div class="tm-placeholder-inset__header"><div class="tm-placeholder__line tm-placeholder__line_inset-header loads"></div></div> <div class="tm-placeholder-inset__body"><ul class="tm-placeholder-list"><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li></ul></div> <div class="tm-placeholder-inset__footer"><div class="tm-placeholder__line tm-placeholder__line_inset-footer loads"></div></div></div> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --></div> <!-- --> <div><div><!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <div class="tm-placeholder-promo"><div class="tm-placeholder-promo__header"><div class="tm-placeholder__line tm-placeholder__line_promo-title"></div></div> <div class="tm-placeholder-promo__body"><div class="tm-placeholder-promo__posts"><div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div> <div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div> <div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div> <div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div> <div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div> <div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div></div> <div class="tm-placeholder-promo__dots"><div class="tm-placeholder-promo__dot"></div> <div class="tm-placeholder-promo__dot"></div> <div class="tm-placeholder-promo__dot"></div></div></div></div> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --></div></div> <section class="tm-block tm-block tm-block_spacing-top" data-async-called="true"><header class="tm-block__header tm-block__header"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title">Работа</h2> </div> <!-- --></header> <div class="tm-block__body tm-block__body"><div class="tm-vacancies-block__item"><a class="tm-vacancies-block__vacancy-title" href="https://career.habr.com/vacancies/data_scientist" target="_blank">
        Data Scientist
      </a> <div class="tm-vacancies-block__vacancies-count">
        116
    вакансий
      </div></div></div> <footer class="tm-block__footer"><a class="tm-block-extralink" href="https://career.habr.com/catalog">
      Все вакансии
    </a></footer></section></div></div></div></div></div> <div class="tm-page__sidebar"><!-- --></div></div></div></div></main> <!-- --></div> <!-- --> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><div class="tm-footer__title"><a class="tm-svg-icon__wrapper tm-footer__title-link router-link-active" href="/ru/"><svg class="tm-svg-img tm-svg-icon" height="16" width="16"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a></div> <div class="tm-footer__social"><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="16" width="16"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="16" width="16"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="16" width="16"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="16" width="16"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="16" width="16"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a class="tm-svg-icon__wrapper tm-social-icons__icon" href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank"><svg class="tm-svg-img tm-svg-icon" height="16" width="16"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <div class="v-portal" style="display:none;"></div> <button class="tm-footer__link"><svg class="tm-svg-img tm-footer__icon" height="16" width="16"><title>Язык</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#lang"></use></svg>
        Настройка языка
      </button> <a class="tm-footer__link" href="/ru/feedback/">
        Техническая поддержка
      </a> <a class="tm-footer__link" href="/berserk-mode-nope">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2023, </span> <span class="tm-copyright__name"><a class="tm-copyright__link" href="https://company.habr.com/" rel="noopener" target="_blank">Habr</a></span></span></div></div></div></div> <!-- --> <!-- --></div> <div class="vue-portal-target"></div></div>





<noscript>
<div>
<img alt="" src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;"/>
</div>
</noscript>


</body>
</html>
